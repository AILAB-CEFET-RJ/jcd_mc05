{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation with ChromaDB\n",
        "\n",
        "## Learning Goals\n",
        "- Understand the architecture of RAG (indexing → retrieval → generation).\n",
        "- Implement a minimal RAG pipeline with LangChain and ChromaDB.\n",
        "- Compare different chunking strategies and their effect on retrieval.\n",
        "- Query the vector database and inject retrieved context into the LLM prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load get_llm.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Prefer the dedicated package; fallback keeps compatibility with older installs.\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "except ImportError:\n",
        "    from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "def get_llm(provider: str = \"openai\"):\n",
        "    provider = provider.strip().lower()\n",
        "\n",
        "    if provider == \"openai\":\n",
        "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "            raise ValueError(\n",
        "                \"OPENAI_API_KEY not found. Add it to your .env before using provider='openai'.\"\n",
        "            )\n",
        "        return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "    if provider == \"ollama\":\n",
        "        return ChatOllama(model=\"gemma2:2b\", temperature=0)\n",
        "\n",
        "    raise ValueError(\"Unsupported provider. Use 'openai' or 'ollama'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# It is assumed that get_llm() is defined elsewhere (Notebook 2 helper)\n",
        "llm = get_llm(\"openai\")  # or get_llm(\"ollama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Load sample documents\n",
        "A small text file is created and used for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded document:\n",
            "\n",
            "Databases are organized collections of structured information or data,\n",
            "typically stored electronically in a computer system.\n",
            "Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "normally require human intelligence, such as reasoning and learning.\n",
            "LLM-based agents combine large language models with external tools.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"\"\"\n",
        "Databases are organized collections of structured information or data,\n",
        "typically stored electronically in a computer system.\n",
        "Artificial intelligence (AI) refers to systems that can perform tasks that\n",
        "normally require human intelligence, such as reasoning and learning.\n",
        "LLM-based agents combine large language models with external tools.\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/sample_doc.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "loader = TextLoader(\"data/sample_doc.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(\"Loaded document:\")\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Chunking\n",
        "The document is split into overlapping chunks to fit into the model context window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 3\n",
            "Chunk 0: Databases are organized collections of structured information or data,\n",
            "typically stored electronically in a computer system.\n",
            "Chunk 1: Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "normally require human intelligence, such as reasoning and learning.\n",
            "Chunk 2: LLM-based agents combine large language models with external tools.\n"
          ]
        }
      ],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=180, chunk_overlap=20)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "print(\"Number of chunks:\", len(chunks))\n",
        "for i, c in enumerate(chunks):\n",
        "    print(f\"Chunk {i}: {c.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — Embedding + Indexing in ChromaDB\n",
        "Each chunk is converted into embeddings and stored in a local ChromaDB instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8221/3743992889.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e7a5048cb67486da0c1b649ff4d4ee4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        }
      ],
      "source": [
        "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": \"cpu\"}\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"data/chroma_store\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Retrieval\n",
        "Semantic search is performed over the vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved chunks:\n",
            "- Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "normally require human intelligence, such as reasoning and learning.\n",
            "- Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "normally require human intelligence, such as reasoning and learning.\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "\n",
        "query = \"What is artificial intelligence?\"\n",
        "results = retriever.invoke(query)\n",
        "\n",
        "print(\"Retrieved chunks:\")\n",
        "for r in results:\n",
        "    print(\"-\", r.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 — Generation with retrieved context\n",
        "The retrieved chunks are injected into a prompt template before calling the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: What is a database?\n",
            "Answer: A database is an organized collection of structured information or data, typically stored electronically in a computer system. It allows for efficient storage, retrieval, and management of data, enabling users to easily access and manipulate the information as needed. Databases can be used for various applications, ranging from simple data storage to complex data management systems.\n",
            "\n",
            "Human: What are LLM-based agents?\n",
            "Answer: LLM-based agents are systems that integrate large language models (LLMs) with external tools and resources to perform various tasks. These agents leverage the natural language processing capabilities of LLMs to understand and generate human-like text, while also utilizing external tools to enhance their functionality. This combination allows them to perform complex operations, such as retrieving information, executing commands, or interacting with other software applications, making them versatile in applications like customer support, content generation, and data analysis.\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Use the following context to answer:\\n\\n{context}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "def rag_pipeline(question: str):\n",
        "    # Retrieve\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Generate\n",
        "    chain = prompt_template | llm\n",
        "    return chain.invoke({\"context\": context, \"question\": question}).content\n",
        "\n",
        "query = \"What is a database?\"\n",
        "print(f\"Human: {query}\\nAnswer: {rag_pipeline(query)}\")\n",
        "print()\n",
        "query = \"What are LLM-based agents?\"\n",
        "print(f\"Human: {query}\\nAnswer: {rag_pipeline(query)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- RAG decouples **knowledge storage** (ChromaDB) from **reasoning** (LLM).\n",
        "- Chunking is critical: too small → fragmented context; too large → exceeds token limits.\n",
        "- The vector database enables **semantic search**, not keyword search.\n",
        "- This architecture is the basis for practical applications like Q&A over documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. Replace the sample document with a **PDF loader** (e.g., `PyPDFLoader`) and index a real article.\n",
        "2. Change the chunk size and observe how retrieval quality changes.\n",
        "3. Experiment with different embedding models (`all-MiniLM`, `multi-qa-mpnet-base-dot-v1`).\n",
        "4. Persist the ChromaDB index and reload it in a new notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ailab311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
