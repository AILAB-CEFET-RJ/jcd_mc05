{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation with ChromaDB\n",
        "\n",
        "## Learning Goals\n",
        "- Understand the architecture of **RAG** (indexing → retrieval → generation).\n",
        "- Implement a minimal RAG pipeline with **LangChain** and **ChromaDB**.\n",
        "- Compare different chunking strategies and their effect on retrieval.\n",
        "- Query the vector database and inject retrieved context into the LLM prompt.\n",
        "\n",
        "This notebook corresponds to Section *1.6 Retrieval-Augmented Generation (RAG)* in the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load get_llm.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Prefer the dedicated package; fallback keeps compatibility with older installs.\n",
        "try:\n",
        "    from langchain_ollama import ChatOllama\n",
        "except ImportError:\n",
        "    from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "def get_llm(provider: str = \"openai\"):\n",
        "    provider = provider.strip().lower()\n",
        "\n",
        "    if provider == \"openai\":\n",
        "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "            raise ValueError(\n",
        "                \"OPENAI_API_KEY not found. Add it to your .env before using provider='openai'.\"\n",
        "            )\n",
        "        return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "    if provider == \"ollama\":\n",
        "        return ChatOllama(model=\"gemma2:2b\", temperature=0)\n",
        "\n",
        "    raise ValueError(\"Unsupported provider. Use 'openai' or 'ollama'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "# !pip install chromadb langchain langchain-community sentence-transformers\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# It is assumed that get_llm() is defined elsewhere (Notebook 2 helper)\n",
        "llm = get_llm(\"openai\")  # or get_llm(\"ollama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Load sample documents\n",
        "A small text file is created and used for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded document:\n",
            "\n",
            "Databases are organized collections of structured information or data,\n",
            "typically stored electronically in a computer system.\n",
            "Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "normally require human intelligence, such as reasoning and learning.\n",
            "LLM-based agents combine large language models with external tools.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"\"\"\n",
        "Databases are organized collections of structured information or data,\n",
        "typically stored electronically in a computer system.\n",
        "Artificial intelligence (AI) refers to systems that can perform tasks that\n",
        "normally require human intelligence, such as reasoning and learning.\n",
        "LLM-based agents combine large language models with external tools.\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/sample_doc.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "loader = TextLoader(\"data/sample_doc.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(\"Loaded document:\")\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Chunking\n",
        "The document is split into overlapping chunks to fit into the model context window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 3\n",
            "Chunk 0: Databases are organized collections of structured information or data,\n",
            "typically stored electronically in a computer system.\n",
            "Chunk 1: Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "normally require human intelligence, such as reasoning and learning.\n",
            "Chunk 2: LLM-based agents combine large language models with external tools.\n"
          ]
        }
      ],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=180, chunk_overlap=20)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "print(\"Number of chunks:\", len(chunks))\n",
        "for i, c in enumerate(chunks):\n",
        "    print(f\"Chunk {i}: {c.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — Embedding + Indexing in ChromaDB\n",
        "Each chunk is converted into embeddings and stored in a local ChromaDB instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 359.34it/s, Materializing param=pooler.dense.weight]                             \n",
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Could not import chromadb python package. Please install it with `pip install chromadb`.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ailab311/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:83\u001b[39m, in \u001b[36mChroma.__init__\u001b[39m\u001b[34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'chromadb'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\u001b[39;00m\n\u001b[32m      2\u001b[39m embedding_model = HuggingFaceEmbeddings(\n\u001b[32m      3\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     model_kwargs={\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m vectorstore = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/chroma_store\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ailab311/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ailab311/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:817\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m    786\u001b[39m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[32m   (...)\u001b[39m\u001b[32m    796\u001b[39m     **kwargs: Any,\n\u001b[32m    797\u001b[39m ) -> Chroma:\n\u001b[32m    798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[32m    799\u001b[39m \n\u001b[32m    800\u001b[39m \u001b[33;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    815\u001b[39m \u001b[33;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[32m    816\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m     chroma_collection = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    827\u001b[39m         ids = [\u001b[38;5;28mstr\u001b[39m(uuid.uuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ailab311/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:238\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    236\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    237\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ailab311/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:86\u001b[39m, in \u001b[36mChroma.__init__\u001b[39m\u001b[34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import chromadb python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install chromadb`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m     )\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m._client_settings = client_settings\n",
            "\u001b[31mImportError\u001b[39m: Could not import chromadb python package. Please install it with `pip install chromadb`."
          ]
        }
      ],
      "source": [
        "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": \"cpu\"}\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"data/chroma_store\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Retrieval\n",
        "Semantic search is performed over the vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved chunks:\n",
            "- Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "- Artificial intelligence (AI) refers to systems that can perform tasks that\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1886745/1810345979.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(query)\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "\n",
        "query = \"What is artificial intelligence?\"\n",
        "results = retriever.get_relevant_documents(query)\n",
        "\n",
        "print(\"Retrieved chunks:\")\n",
        "for r in results:\n",
        "    print(\"-\", r.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 — Generation with retrieved context\n",
        "The retrieved chunks are injected into a prompt template before calling the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A database is an organized collection of structured information or data. It allows for efficient storage, retrieval, and management of data, often using a database management system (DBMS). Databases can store various types of information and are commonly used in applications ranging from business operations to personal data management.\n",
            "LLM-based agents are systems that integrate large language models (LLMs) with external tools and resources to perform various tasks. These agents leverage the natural language processing capabilities of LLMs to understand and generate human-like text, while also utilizing external tools to enhance their functionality. This combination allows them to perform complex operations, such as retrieving information, executing commands, or interacting with other software applications, making them versatile in applications like customer support, content generation, and data analysis.\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Use the following context to answer:\\n\\n{context}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "def rag_pipeline(question: str):\n",
        "    # Retrieve\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Generate\n",
        "    chain = prompt_template | llm\n",
        "    return chain.invoke({\"context\": context, \"question\": question}).content\n",
        "\n",
        "print(rag_pipeline(\"What is a database?\"))\n",
        "print(rag_pipeline(\"What are LLM-based agents?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- RAG decouples **knowledge storage** (ChromaDB) from **reasoning** (LLM).\n",
        "- Chunking is critical: too small → fragmented context; too large → exceeds token limits.\n",
        "- The vector database enables **semantic search**, not keyword search.\n",
        "- This architecture is the basis for practical applications like Q&A over documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. Replace the sample document with a **PDF loader** (e.g., `PyPDFLoader`) and index a real article.\n",
        "2. Change the chunk size and observe how retrieval quality changes.\n",
        "3. Experiment with different embedding models (`all-MiniLM`, `multi-qa-mpnet-base-dot-v1`).\n",
        "4. Persist the ChromaDB index and reload it in a new notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ailab311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
